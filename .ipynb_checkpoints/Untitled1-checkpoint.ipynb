{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b800842d-5e8c-41d2-a732-73a571450770",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basic MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=256, output_dim=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.view(x.size(0), -1))\n",
    "\n",
    "# Fisher Estimator\n",
    "def estimate_fisher(model, dataset, criterion, sample_size=1024):\n",
    "    model.eval()\n",
    "    fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters()}\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i * 128 >= sample_size: break\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        model.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        for n, p in model.named_parameters():\n",
    "            fisher[n] += (p.grad ** 2) / sample_size\n",
    "    return fisher\n",
    "\n",
    "# Online EWC penalty\n",
    "def ewc_loss(model, fisher, prev_params, lambda_):\n",
    "    loss = 0.0\n",
    "    for (n, p), prev_p in zip(model.named_parameters(), prev_params):\n",
    "        loss += (fisher[n] * (p - prev_p).pow(2)).sum()\n",
    "    return lambda_ * loss\n",
    "\n",
    "# Task data with pixel permutations\n",
    "def get_permuted_mnist(task_id, seed=0):\n",
    "    torch.manual_seed(seed + task_id)\n",
    "    perm = torch.randperm(28*28)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.view(-1)[perm].view(1, 28, 28))\n",
    "    ])\n",
    "    train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    return train, test\n",
    "\n",
    "# Training loop for each task\n",
    "def train_task(model, trainset, testsets, ewc_data, use_ewc=False, lambda_=0.4, gamma=1.0, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            if use_ewc and ewc_data:\n",
    "                loss += ewc_loss(model, ewc_data['fisher'], ewc_data['params'], lambda_)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Save current parameters\n",
    "    new_params = [p.detach().clone() for p in model.parameters()]\n",
    "    fisher = estimate_fisher(model, trainset, criterion)\n",
    "\n",
    "    if ewc_data:\n",
    "        # Incremental EWC update\n",
    "        for n in fisher:\n",
    "            fisher[n] = fisher[n] + gamma * ewc_data['fisher'][n]\n",
    "    return {'params': new_params, 'fisher': fisher}\n",
    "\n",
    "# Evaluate on all tasks\n",
    "def evaluate(model, testsets):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for testset in testsets:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loader = DataLoader(testset, batch_size=256)\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "                pred = model(x).argmax(1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        accs.append(correct / total)\n",
    "    return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7d880-333e-47a6-b9d0-0c8b9759cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "torch.manual_seed(0)\n",
    "num_tasks = 5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Two models: baseline and with EWC\n",
    "model_base = MLP().to(device)\n",
    "model_ewc = MLP().to(device)\n",
    "ewc_data = None\n",
    "\n",
    "results_base = []\n",
    "results_ewc = []\n",
    "testsets = []\n",
    "\n",
    "for task_id in range(num_tasks):\n",
    "    print(f\"\\n=== Task {task_id+1} ===\")\n",
    "    trainset, testset = get_permuted_mnist(task_id)\n",
    "    testsets.append(testset)\n",
    "\n",
    "    # Train baseline\n",
    "    train_task(model_base, trainset, testsets, None, use_ewc=False)\n",
    "    acc_base = evaluate(model_base, testsets)\n",
    "    results_base.append(acc_base)\n",
    "\n",
    "    # Train EWC model\n",
    "    ewc_data = train_task(model_ewc, trainset, testsets, ewc_data, use_ewc=True)\n",
    "    acc_ewc = evaluate(model_ewc, testsets)\n",
    "    results_ewc.append(acc_ewc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8827182f-0227-4b10-9ef6-f4a3f3a55143",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base = np.array(results_base)\n",
    "results_ewc = np.array(results_ewc)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for t in range(num_tasks):\n",
    "    plt.plot(range(t+1, num_tasks+1), results_base[t:, t], label=f'Base: Task {t+1}')\n",
    "    plt.plot(range(t+1, num_tasks+1), results_ewc[t:, t], '--', label=f'EWC: Task {t+1}')\n",
    "plt.xlabel('Task index (after)')\n",
    "plt.ylabel('Accuracy on Task')\n",
    "plt.title('Continual Learning: Baseline vs EWC')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
